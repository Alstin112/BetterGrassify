{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "284f279d-7ff5-4926-84c7-8952ab8718d5",
      "metadata": {
        "id": "284f279d-7ff5-4926-84c7-8952ab8718d5"
      },
      "source": [
        "# Aula 04: Classificação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ac64c6e9-ee5c-4741-b0a2-ae8a5c6d7aa5",
      "metadata": {
        "id": "ac64c6e9-ee5c-4741-b0a2-ae8a5c6d7aa5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29eb6d53-6463-4843-8cbd-303f7ce48e1a",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "29eb6d53-6463-4843-8cbd-303f7ce48e1a"
      },
      "source": [
        "# Sumário"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b61bae59-a732-4328-a254-ea7624f86341",
      "metadata": {
        "id": "b61bae59-a732-4328-a254-ea7624f86341"
      },
      "source": [
        "## Objetivos\n",
        "\n",
        "1. Implementar o método de regressão logística usado em problemas de classificação;\n",
        "2. Implementar uma extensão do método fazendo regularização com a norma $L_2$ (i.e. _Ridge regression_).\n",
        "\n",
        "## Exercícios\n",
        "\n",
        "### Parte 1: Regressão Logística\n",
        "- Exercício 1: Preparação dos dados\n",
        "- Exercício 2: Regressão logística - método do gradiente\n",
        "- Exercício 3: Regressão logística - scikit-learn\n",
        "\n",
        "### Parte 2: Regularização\n",
        "- Exercício 4: Preparação dos dados\n",
        "- Exercício 5: Regressão logística com regularização - método do gradiente\n",
        "- Exercício 6: Regressão logística com regularização - scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "382a8963-2456-4acc-aa9f-736c3b4e74c0",
      "metadata": {
        "id": "382a8963-2456-4acc-aa9f-736c3b4e74c0"
      },
      "source": [
        "# Parte 1: Regressão logística"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0733a177-360c-40a3-b120-0b328ecbc637",
      "metadata": {
        "id": "0733a177-360c-40a3-b120-0b328ecbc637",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Revisão teórica"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb6109e2-d1f9-47c8-b771-a369ac84aaa9",
      "metadata": {
        "id": "bb6109e2-d1f9-47c8-b771-a369ac84aaa9"
      },
      "source": [
        "### Motivação\n",
        "Muitos problemas práticos de aprendizado de máquina envolvem **classificação binária**, isto é, prever uma entre duas categorias (por exemplo, **sim/não**, **0/1**, **sucesso/falha**). Nesses casos, não faz sentido usar regressão linear convencional, pois ela produz valores contínuos não limitados, podendo extrapolar além do intervalo [0,1]. Para problemas de classificação, desejamos em vez disso estimar diretamente a **probabilidade** de um evento binário ocorrer, dado um vetor de atributos de entrada.\n",
        "\n",
        "### Modelo de Regressão Logística\n",
        "A **regressão logística** modela diretamente a probabilidade de um evento binário e permite definir um critério para obter uma fronteira de decisão bem definida. Para obter saídas probabilísticas, aplicamos a função sigmoide a uma combinação linear:\n",
        "\n",
        "$$\n",
        "f_{\\mathbf{w},b}(\\mathbf{x}) = g(\\mathbf{w}\\cdot \\mathbf{x} + b)\\,, \\tag{1}\n",
        "$$\n",
        "$$\n",
        "g(z) = \\frac{1}{\\,1 + e^{-z}\\,}\\,. \\tag{2}\n",
        "$$\n",
        "A sigmoide é monotônica e mapeia $\\mathbb{R}$ para $(0,1)$; logo, $f_{\\mathbf{w},b}(\\mathbf{x})$ é sempre interpretável como uma probabilidade. O hiperplano $\\mathbf{w}\\cdot\\mathbf{x}+b=0$ corresponde a $f_{\\mathbf{w},b}=0.5$.\n",
        "\n",
        "### Função de Custo\n",
        "Ajustamos $(\\mathbf{w}, b)$ minimizando a média da perda logarítmica binária, apropriada para alvos Bernoulli:\n",
        "$$\n",
        "J(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\Big[ \\text{loss}\\!\\big(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}),\\, y^{(i)}\\big) \\Big] \\tag{3}\n",
        "$$\n",
        "$$\n",
        "\\text{loss}\\big(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}),\\, y^{(i)}\\big) = -\\,y^{(i)} \\log\\!\\Big(f_{\\mathbf{w},b}\\big( \\mathbf{x}^{(i)} \\big)\\Big)\\;-\\;\\big(1 - y^{(i)}\\big)\\,\\log\\!\\Big(1 - f_{\\mathbf{w},b}\\big( \\mathbf{x}^{(i)} \\big)\\Big)\\,. \\tag{4}\n",
        "$$\n",
        "\n",
        "Se $y^{(i)}=1$, o segundo termo se anula; já caso $y^{(i)}=0$, o primeiro termo se anula.\n",
        "\n",
        "### Gradiente da Função de Custo\n",
        "As derivadas parciais podem ser obtidos com a regra da cadeia e possuem a seguinte forma:\n",
        "$$\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} = \\frac{1}{m}\\sum_{i=0}^{m-1} \\Big(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}\\Big)\\,x_{j}^{(i)}\\,, \\quad \\text{para cada } j=0,1,\\dots,n-1\\,. \\tag{5a}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b} = \\frac{1}{m}\\sum_{i=0}^{m-1} \\Big(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}\\Big)\\,, \\tag{5b}\n",
        "$$\n",
        "As expressões acima revelam que o gradiente do custo em relação a cada peso $w_j$ é proporcional à média dos erros *$(\\text{previsão} - \\text{valor real})$* multiplicados pelo respectivo atributo $x_j$. Já o gradiente em relação a $b$ (termo de bias) é a média simples dos erros $(f_{\\mathbf{w},b}(\\mathbf{x}) - y)$.\n",
        "\n",
        "**Embora a forma lembre a da regressão linear, aqui $f_{\\mathbf{w},b}$ é sigmoidal.**\n",
        "\n",
        "### Ajuste de Parâmetros com o Método do Gradiente\n",
        "Para minimizar $J(\\mathbf{w},b)$ de forma iterativa, utilizamos o **método do gradiente**. Inicializamos os pesos $\\mathbf{w}$ e bias $b$ (por exemplo, com zeros ou valores pequenos aleatórios) e então atualizamos os parâmetros repetidamente na direção oposta ao gradiente, com um passo proporcional à **taxa de aprendizado** $\\alpha > 0$. A regra de atualização em cada iteração é dada por:\n",
        "$$\n",
        "\\begin{align*}\n",
        "& \\text{Repeat } \\{ \\\\\n",
        "& \\quad b \\;:=\\; b \\;-\\; \\alpha \\;\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}, \\tag{6a} \\\\\n",
        "& \\quad w_j \\;:=\\; w_j \\;-\\; \\alpha \\;\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}, \\quad \\text{para } j = 0,\\ldots,n-1. \\tag{6b}\\\\\n",
        "& \\}\n",
        "\\end{align*}\n",
        "$$\n",
        "Esse procedimento é repetido iterativamente até que $J(\\mathbf{w},b)$ convirja para um mínimo (ou até atingir um número máximo de épocas).\n",
        "\n",
        "### Interpretação Probabilística e Regra de Decisão\n",
        "Um dos principais atrativos da regressão logística é a sua **interpretação probabilística**. A saída do modelo $f_{\\mathbf{w},b}(\\mathbf{x})$ pode ser entendida como a probabilidade estimada de $y=1$ dado o vetor de atributos $\\mathbf{x}$ – isto é, $f_{\\mathbf{w},b}(\\mathbf{x}) \\approx P(y=1 \\mid \\mathbf{x})$. Por exemplo, se $f_{\\mathbf{w},b}(\\mathbf{x}) = 0.85$, o modelo estima **85% de chance** de o exemplo pertença à classe positiva ($y=1$). Essa interpretação é coerente com a forma funcional do modelo: estamos basicamente ajustando uma distribuição de Bernoulli, cuja média (probabilidade de sucesso) depende de $\\mathbf{x}$ através da função sigmoide.\n",
        "\n",
        "Para utilizar o modelo em tarefas de classificação, é preciso converter a probabilidade prevista em um **rótulo binário** $\\hat{y}$. A regra de decisão mais comum é aplicar um **limiar de 0.5**:\n",
        "\n",
        "- Se $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) \\geq 0.5$, então classificamos $\\hat{y}^{(i)} = 1$ (classe positiva).  \n",
        "- Caso contrário ($f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) < 0.5$), classificamos $\\hat{y}^{(i)} = 0$ (classe negativa).\n",
        "\n",
        "### Resumo\n",
        "A regressão logística combina um modelo linear com a sigmoide para produzir probabilidades em $(0,1)$, otimizadas via perda logarítmica e gradiente descendente, resultando em um classificador binário simples, interpretável e eficaz.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "doCpahQBbxpG",
      "metadata": {
        "id": "doCpahQBbxpG",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Exercício 1: Preparação dos dados"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "N6Oki-S1ncc1",
      "metadata": {
        "id": "N6Oki-S1ncc1"
      },
      "source": [
        "Como no notebook da Aula 3, vamos usar dados simplificados. Estes dados descrevem o resultado do processo de admissão em uma universidade americana fictícia, em função das notas obtidas em dois exames distintos. Cada linha corresponde a um estudante e contém 3 colunas: duas colunas com notas numéricas dos exames 1 e 2, e uma terceira coluna com uma variável binária (0/1), onde 1 significa que o estudante foi aceito."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dd16628-3c51-4ff1-b5d7-e57fdd259c1d",
      "metadata": {
        "id": "1dd16628-3c51-4ff1-b5d7-e57fdd259c1d"
      },
      "source": [
        "### Criando o conjunto de dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fdd3ec27-27cb-4f70-843e-0b5c4f5914df",
      "metadata": {
        "id": "fdd3ec27-27cb-4f70-843e-0b5c4f5914df"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ./data\n",
        "!touch ./data/admissions.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4b1191f7-aa83-4816-a076-a24d92fbd4fd",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "4b1191f7-aa83-4816-a076-a24d92fbd4fd"
      },
      "outputs": [],
      "source": [
        "admissions_data = \"\"\"34.62365962451697,78.0246928153624,0\n",
        "30.28671076822607,43.89499752400101,0\n",
        "35.84740876993872,72.90219802708364,0\n",
        "60.18259938620976,86.30855209546826,1\n",
        "79.0327360507101,75.3443764369103,1\n",
        "45.08327747668339,56.3163717815305,0\n",
        "61.10666453684766,96.51142588489624,1\n",
        "75.02474556738889,46.55401354116538,1\n",
        "76.09878670226257,87.42056971926803,1\n",
        "84.43281996120035,43.53339331072109,1\n",
        "95.86155507093572,38.22527805795094,0\n",
        "75.01365838958247,30.60326323428011,0\n",
        "82.30705337399482,76.48196330235604,1\n",
        "69.36458875970939,97.71869196188608,1\n",
        "39.53833914367223,76.03681085115882,0\n",
        "53.9710521485623,89.20735013750205,1\n",
        "69.07014406283025,52.74046973016765,1\n",
        "67.94685547711617,46.67857410673128,0\n",
        "70.66150955499435,92.92713789364831,1\n",
        "76.97878372747498,47.57596364975532,1\n",
        "67.37202754570876,42.83843832029179,0\n",
        "89.67677575072079,65.79936592745237,1\n",
        "50.534788289883,48.85581152764205,0\n",
        "34.21206097786789,44.20952859866288,0\n",
        "77.9240914545704,68.9723599933059,1\n",
        "62.27101367004632,69.95445795447587,1\n",
        "80.1901807509566,44.82162893218353,1\n",
        "93.114388797442,38.80067033713209,0\n",
        "61.83020602312595,50.25610789244621,0\n",
        "38.78580379679423,64.99568095539578,0\n",
        "61.379289447425,72.80788731317097,1\n",
        "85.40451939411645,57.05198397627122,1\n",
        "52.10797973193984,63.12762376881715,0\n",
        "52.04540476831827,69.43286012045222,1\n",
        "40.23689373545111,71.16774802184875,0\n",
        "54.63510555424817,52.21388588061123,0\n",
        "33.91550010906887,98.86943574220611,0\n",
        "64.17698887494485,80.90806058670817,1\n",
        "74.78925295941542,41.57341522824434,0\n",
        "34.1836400264419,75.2377203360134,0\n",
        "83.90239366249155,56.30804621605327,1\n",
        "51.54772026906181,46.85629026349976,0\n",
        "94.44336776917852,65.56892160559052,1\n",
        "82.36875375713919,40.61825515970618,0\n",
        "51.04775177128865,45.82270145776001,0\n",
        "62.22267576120188,52.06099194836679,0\n",
        "77.19303492601364,70.45820000180959,1\n",
        "97.77159928000232,86.7278223300282,1\n",
        "62.07306379667647,96.76882412413983,1\n",
        "91.56497449807442,88.69629254546599,1\n",
        "79.94481794066932,74.16311935043758,1\n",
        "99.2725269292572,60.99903099844988,1\n",
        "90.54671411399852,43.39060180650027,1\n",
        "34.52451385320009,60.39634245837173,0\n",
        "50.2864961189907,49.80453881323059,0\n",
        "49.58667721632031,59.80895099453265,0\n",
        "97.64563396007767,68.86157272420604,1\n",
        "32.57720016809309,95.59854761387875,0\n",
        "74.24869136721598,69.82457122657193,1\n",
        "71.79646205863379,78.45356224515052,1\n",
        "75.3956114656803,85.75993667331619,1\n",
        "35.28611281526193,47.02051394723416,0\n",
        "56.25381749711624,39.26147251058019,0\n",
        "30.05882244669796,49.59297386723685,0\n",
        "44.66826172480893,66.45008614558913,0\n",
        "66.56089447242954,41.09209807936973,0\n",
        "40.45755098375164,97.53518548909936,1\n",
        "49.07256321908844,51.88321182073966,0\n",
        "80.27957401466998,92.11606081344084,1\n",
        "66.74671856944039,60.99139402740988,1\n",
        "32.72283304060323,43.30717306430063,0\n",
        "64.0393204150601,78.03168802018232,1\n",
        "72.34649422579923,96.22759296761404,1\n",
        "60.45788573918959,73.09499809758037,1\n",
        "58.84095621726802,75.85844831279042,1\n",
        "99.82785779692128,72.36925193383885,1\n",
        "47.26426910848174,88.47586499559782,1\n",
        "50.45815980285988,75.80985952982456,1\n",
        "60.45555629271532,42.50840943572217,0\n",
        "82.22666157785568,42.71987853716458,0\n",
        "88.9138964166533,69.80378889835472,1\n",
        "94.83450672430196,45.69430680250754,1\n",
        "67.31925746917527,66.58935317747915,1\n",
        "57.23870631569862,59.51428198012956,1\n",
        "80.36675600171273,90.96014789746954,1\n",
        "68.46852178591112,85.59430710452014,1\n",
        "42.0754545384731,78.84478600148043,0\n",
        "75.47770200533905,90.42453899753964,1\n",
        "78.63542434898018,96.64742716885644,1\n",
        "52.34800398794107,60.76950525602592,0\n",
        "94.09433112516793,77.15910509073893,1\n",
        "90.44855097096364,87.50879176484702,1\n",
        "55.48216114069585,35.57070347228866,0\n",
        "74.49269241843041,84.84513684930135,1\n",
        "89.84580670720979,45.35828361091658,1\n",
        "83.48916274498238,48.38028579728175,1\n",
        "42.2617008099817,87.10385094025457,1\n",
        "99.31500880510394,68.77540947206617,1\n",
        "55.34001756003703,64.9319380069486,1\n",
        "74.77589300092767,89.52981289513276,1\"\"\"\n",
        "\n",
        "with open('./data/admissions.csv', 'w') as f:\n",
        "    f.write(admissions_data)\n",
        "\n",
        "# Importando dados\n",
        "\n",
        "X_features = ['Exam 1 score', 'Exam 2 score']\n",
        "y_name = ['Admitted']\n",
        "data = pd.read_csv(\"./data/admissions.csv\", header=None, names=X_features + y_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2584caaa-9cb4-4e1c-8031-021f012604b8",
      "metadata": {
        "id": "2584caaa-9cb4-4e1c-8031-021f012604b8"
      },
      "source": [
        "### 1.a) Crie a matriz de treino $X_{\\rm train}$, o vetor de variáveis alvo $y_{\\rm train}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "042f3e65-a91d-491b-b924-42e9dcfd364b",
      "metadata": {
        "id": "042f3e65-a91d-491b-b924-42e9dcfd364b"
      },
      "outputs": [],
      "source": [
        "# ---- INSIRA SEU CÓDIGO AQUI\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_data = data.iloc[:,0:2]\n",
        "y_data = data.iloc[:,2]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.30, random_state=42)\n",
        "# ----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fbedc18-183a-45c8-83cb-812ddab7dcfd",
      "metadata": {
        "id": "4fbedc18-183a-45c8-83cb-812ddab7dcfd"
      },
      "source": [
        "### 1b) Explore (rapidamente) os dados"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8757a203-2422-440e-96ee-8bcde5deca90",
      "metadata": {
        "id": "8757a203-2422-440e-96ee-8bcde5deca90"
      },
      "source": [
        "**Inspecione a tabela com funcionalidade da biblioteca pandas e faça um scatterplot dos dados, usando a coluna \"Admitted\" como cor do marcador.**\n",
        "\n",
        "_Obs: não gaste muito tempo neste exercício. Por outro lado, é sempre importante inspecionar rapidamente os dados._\n",
        "\n",
        "\n",
        "> Besides `display()`, you can use several other methods to inspect a pandas DataFrame:\n",
        ">\n",
        "> - `.head()`: Shows the first 5 rows (or a specified number) of the DataFrame. Useful for a quick look at the data structure.\n",
        "> - `.info()`: Prints a concise summary of the DataFrame, including the index dtype > and column dtypes, non-null values and memory usage.\n",
        "> - `.describe()`: Generates descriptive statistics of the DataFrame's numerical > columns, such as count, mean, standard deviation, minimum, and maximum.\n",
        "> - `.shape`: Returns a tuple representing the dimensionality of the DataFrame (rows, > columns)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "885cf223-a8db-4a21-aa5e-3efca5a68760",
      "metadata": {
        "id": "885cf223-a8db-4a21-aa5e-3efca5a68760"
      },
      "outputs": [],
      "source": [
        "# ---- INSIRA SEU CÓDIGO AQUI\n",
        "# ----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbc1b003-bf09-4c18-8f22-1b12c8d12ca0",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "fbc1b003-bf09-4c18-8f22-1b12c8d12ca0"
      },
      "source": [
        "## Exercício 2: Regressão logística - Método do gradiente"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ab34091-3671-4731-a482-3bfbdff5c282",
      "metadata": {
        "id": "5ab34091-3671-4731-a482-3bfbdff5c282"
      },
      "source": [
        "O objetivo deste exercício é implementar o método do gradiente para regressão logística."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa515c6e-9948-48b7-8609-c6e2d12f2695",
      "metadata": {
        "id": "aa515c6e-9948-48b7-8609-c6e2d12f2695"
      },
      "source": [
        "### 2.a) Implemente as funções do método do gradiente em regressão logística"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b71d8ca-73bf-4077-a6db-8478a2baa498",
      "metadata": {
        "id": "1b71d8ca-73bf-4077-a6db-8478a2baa498"
      },
      "source": [
        "Será necessário definir as seguintes funções:\n",
        "\n",
        "- `sigmoid` define a função sigmóide\n",
        "- `compute_cost` implementa a equação (3) acima;\n",
        "- `compute_gradient` implementa as equações (5a) e (5b) acima;\n",
        "- `gradient_descent` implementa o método do gradiente segundo as equações (6a) e (6b) acima;\n",
        "- `predict` implementa o critério de corte que define a etiqueta prevista pelo modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "id": "b5faae43-3770-4d3c-9465-98c9a6bacb83",
      "metadata": {
        "id": "b5faae43-3770-4d3c-9465-98c9a6bacb83"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of z\n",
        "\n",
        "    Args:\n",
        "        z (ndarray): A scalar, numpy array of any size.\n",
        "\n",
        "    Returns:\n",
        "        g (ndarray): sigmoid(z), with the same shape as z\n",
        "    \"\"\"\n",
        "\n",
        "    # ---- INSIRA SEU CÓDIGO AQUI\n",
        "    g = 1/(1+np.exp(-z))\n",
        "    # ----\n",
        "\n",
        "    return g\n",
        "\n",
        "\n",
        "def f_wb(X, w, b):\n",
        "  return sigmoid(w @ X.T + b)\n",
        "\n",
        "def compute_cost(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the cost $J(\\vec{w}, b)$ for a logistic regression model.\n",
        "\n",
        "    Args:\n",
        "      X (ndarray (m, n)): Data, m examples with n features each\n",
        "      y (ndarray (m,))  : target values\n",
        "      w (ndarray (n,))  : model weight parameters\n",
        "      b (scalar)        : model bias parameter\n",
        "\n",
        "    Returns:\n",
        "      cost (scalar): cost\n",
        "    \"\"\"\n",
        "\n",
        "    # ---- INSIRA SEU CÓDIGO AQUI\n",
        "    m = len(y)\n",
        "    f = f_wb(X,w,b)\n",
        "    losses = -y * np.log(f) - (1-y)*np.log(1-f)\n",
        "\n",
        "    cost = losses.sum()/m\n",
        "    # ----\n",
        "\n",
        "    return cost\n",
        "\n",
        "\n",
        "def compute_gradient(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the gradient for logistic regression\n",
        "    Args:\n",
        "      X (ndarray (m, n)): Data, m examples with n features each\n",
        "      y (ndarray (m,))  : target values\n",
        "      w (ndarray (n,))  : model weight parameters\n",
        "      b (scalar)        : model bias parameter\n",
        "\n",
        "    Returns:\n",
        "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.\n",
        "      dj_db (scalar)      : The gradient of the cost w.r.t. the parameter b.\n",
        "    \"\"\"\n",
        "\n",
        "    # ---- INSIRA SEU CÓDIGO AQUI\n",
        "    m = len(y)\n",
        "    dif = f_wb(X,w,b) - y\n",
        "    dj_dw = 1/m * (X.T @ dif)\n",
        "    dj_db = 1/m * dif.sum()\n",
        "    # ----\n",
        "\n",
        "    return dj_dw, dj_db\n",
        "\n",
        "\n",
        "def gradient_descent(X, y, w_in, b_in, alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Performs batch gradient descent to learn w and b. Updates w and b by taking\n",
        "    num_iters gradient steps with learning rate alpha\n",
        "\n",
        "    Args:\n",
        "      X (ndarray (m, n)) : Data, m examples with n features each\n",
        "      y (ndarray (m,))   : target values\n",
        "      w_in (ndarray (n,)): initial model weight parameters\n",
        "      b_in (scalar)      : initial model bias parameter\n",
        "      alpha (float)      : Learning rate\n",
        "      num_iters (int)    : number of iterations to run gradient descent\n",
        "\n",
        "    Returns:\n",
        "      w (ndarray (n,))                   : Updated values of parameters\n",
        "      b (scalar)                         : Updated value of parameter\n",
        "      J_history (ndarray (num_iters,))   : History of cost values\n",
        "      w_history (ndarray (num_iters, n)) : History of parameters w\n",
        "      b_history (ndarray (num_iters,))   : History of parameters b\n",
        "      \"\"\"\n",
        "\n",
        "    # ---- INSIRA SEU CÓDIGO AQUI\n",
        "    w = w_in\n",
        "    b = b_in\n",
        "    w_history = [w]\n",
        "    b_history = [b]\n",
        "    J_history = [compute_cost(X,y,w,b)]\n",
        "\n",
        "    for i in range(num_iters):\n",
        "      dj_dw, dj_db = compute_gradient(X,y,w,b)\n",
        "      w -= alpha*dj_dw\n",
        "      b -= alpha*dj_db\n",
        "\n",
        "      w_history.append(w)\n",
        "      b_history.append(b)\n",
        "      J_history.append(compute_cost(X,y,w,b))\n",
        "    # ----\n",
        "\n",
        "    return w, b, J_history, w_history, b_history\n",
        "\n",
        "\n",
        "def predict(X, w, b):\n",
        "    \"\"\"\n",
        "    Predict whether the label is 0 or 1 using learned logistic\n",
        "    regression parameters w\n",
        "\n",
        "    Args:\n",
        "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
        "      w : (ndarray Shape (n,))  values of parameters of the model\n",
        "      b : (scalar)              value of bias parameter of the model\n",
        "\n",
        "    Returns:\n",
        "      p : (ndarray (m,)) The predictions for X using a threshold at 0.5\n",
        "    \"\"\"\n",
        "\n",
        "    # ---- INSIRA SEU CÓDIGO AQUI\n",
        "    y_pred = (sigmoid(w @ X.T + b)> 0.5).astype(int)\n",
        "    # ----\n",
        "\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbfee876-fa88-426d-82f3-235ebfa72469",
      "metadata": {
        "id": "dbfee876-fa88-426d-82f3-235ebfa72469"
      },
      "source": [
        "### 2.b) Treine o modelo, faça previsões e inspecione graficamente os resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18b08e02-7fb3-4447-8272-9fc74a4839a9",
      "metadata": {
        "id": "18b08e02-7fb3-4447-8272-9fc74a4839a9"
      },
      "source": [
        "Faça a regressão logística nos dados e verifique graficamente que o comportamento da função de custo está como o esperado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "id": "3b1bc73b-c8cf-4326-9572-42de644c18b9",
      "metadata": {
        "id": "3b1bc73b-c8cf-4326-9572-42de644c18b9",
        "outputId": "a1963de9-0516-4fca-a69b-db4d2ad2eeaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exam 1 score    0.953258\n",
            "Exam 2 score    0.951912\n",
            "dtype: float64 9.999114285714299\n",
            "CPU times: user 68.5 ms, sys: 0 ns, total: 68.5 ms\n",
            "Wall time: 69.6 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7e49d911ac00>"
            ]
          },
          "metadata": {},
          "execution_count": 159
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# ---- INSIRA SEU CÓDIGO AQUI\n",
        "w_in = 1*np.ones(X_train.shape[1])\n",
        "b_in = 10\n",
        "alpha = 0.0001\n",
        "num_iters = 20\n",
        "w, b, J_history, w_history, b_history = gradient_descent(X_train, y_train, w_in, b_in, alpha, num_iters)\n",
        "#plt.plot(J_history)\n",
        "df_zeros = X_train[y_train.iloc[:]==0]\n",
        "df_ones = X_train[y_train.iloc[:]==1]\n",
        "\n",
        "#plt.scatter(df_zeros.iloc[:,0], df_zeros.iloc[:,1])\n",
        "#plt.scatter(df_ones.iloc[:,0], df_ones.iloc[:,1])\n",
        "\n",
        "print(w,b)\n",
        "predicted = predict(X_test,[0.1,0.1],-10)\n",
        "\n",
        "p_zeros = X_test[predicted.iloc[:]==0]\n",
        "p_ones = X_test[predicted.iloc[:]==1]\n",
        "\n",
        "plt.scatter(p_zeros.iloc[:,0], p_zeros.iloc[:,1])\n",
        "plt.scatter(p_ones.iloc[:,0], p_ones.iloc[:,1])\n",
        "# ----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e26f8e5-7556-409c-bb0c-4fa03acf282e",
      "metadata": {
        "id": "9e26f8e5-7556-409c-bb0c-4fa03acf282e",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Exercício 3: Regressão logística - Scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c64ebd10-8431-467a-b0e9-168bbaf031b8",
      "metadata": {
        "id": "c64ebd10-8431-467a-b0e9-168bbaf031b8"
      },
      "source": [
        "O objetivo deste exercício é obter os memos resultados do exercício acima usando a funcionalidade já existente no scikit-learn (`LogisticRegression`). Calcule também a acurácia usando a função `score` e faça um `plt.scatter` com código de cor correspondendo a sucesso/fracasso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "fbc3c9ce-279a-4145-82ab-ee34b86847b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbc3c9ce-279a-4145-82ab-ee34b86847b5",
        "outputId": "5f79a03b-e6dc-4945-ddf0-6040ab8d00b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1]\n"
          ]
        }
      ],
      "source": [
        "# ---- INSIRA SEU CÓDIGO AQUI\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(y_pred)\n",
        "# ----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ed13176-40e6-4483-8cd5-93b56b522bdc",
      "metadata": {
        "id": "2ed13176-40e6-4483-8cd5-93b56b522bdc"
      },
      "source": [
        "# Parte 2: Regularização"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c6f9a34-696f-48b8-8c9e-626aa26dac1e",
      "metadata": {
        "id": "8c6f9a34-696f-48b8-8c9e-626aa26dac1e",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Revisão teórica"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c352d18b-0a53-4a2d-ae7e-edcb9606de1a",
      "metadata": {
        "id": "c352d18b-0a53-4a2d-ae7e-edcb9606de1a"
      },
      "source": [
        "### Motivação\n",
        "Modelos de classificação tendem a **sobreajustar** quando há muitas *features* ou correlações fortes. A **regularização** introduz um termo de penalização no objetivo para controlar a complexidade do modelo, reduzindo variância e melhorando a generalização, sem alterar a forma do classificador logístico.\n",
        "\n",
        "### Modelo de Regressão Logística (inalterado)\n",
        "A predição probabilística continua sendo dada pela composição de uma combinação linear com a sigmoide:\n",
        "$$\n",
        "f_{\\mathbf{w},b}(\\mathbf{x}) = g(\\mathbf{w}\\cdot \\mathbf{x} + b)\\,, \\tag{1}\n",
        "$$\n",
        "$$\n",
        "g(z) = \\frac{1}{\\,1 + e^{-z}\\,}\\,. \\tag{2}\n",
        "$$\n",
        "\n",
        "### Função de Custo Regularizada\n",
        "A regularização modifica a função de custo adicionando uma penalização aos pesos. Em **L2 (Ridge)**, penalizamos a norma quadrática dos pesos (não se regulariza o viés $b$):\n",
        "$$\n",
        "J_\\lambda(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\Big[ \\text{loss}\\!\\big(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}),\\, y^{(i)}\\big) \\Big] \\;+\\; \\frac{\\lambda}{2m}\\,\\|\\mathbf{w}\\|_2^2\\,. \\tag{3}\n",
        "$$\n",
        "Alternativamente, em **L1 (Lasso)**:\n",
        "$$\n",
        "J_\\lambda^{(L1)}(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\Big[ \\text{loss}\\!\\big(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}),\\, y^{(i)}\\big) \\Big] \\;+\\; \\frac{\\lambda}{m}\\,\\|\\mathbf{w}\\|_1\\,. \\tag{4}\n",
        "$$\n",
        "Aqui, a perda logarítmica binária permanece:\n",
        "$$\n",
        "\\text{loss}\\big(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}),\\, y^{(i)}\\big) = -\\,y^{(i)} \\log\\!\\Big(f_{\\mathbf{w},b}\\big( \\mathbf{x}^{(i)} \\big)\\Big)\\;-\\;\\big(1 - y^{(i)}\\big)\\,\\log\\!\\Big(1 - f_{\\mathbf{w},b}\\big( \\mathbf{x}^{(i)} \\big)\\Big)\\,. \\tag{5}\n",
        "$$\n",
        "\n",
        "### Gradientes (L2) e Subgradientes (L1)\n",
        "Para **L2**, os gradientes tornam-se:\n",
        "$$\n",
        "\\frac{\\partial J_\\lambda(\\mathbf{w},b)}{\\partial w_j} = \\frac{1}{m}\\sum_{i=0}^{m-1} \\Big(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}\\Big)\\,x_{j}^{(i)} \\;+\\; \\frac{\\lambda}{m}\\,w_j\\,, \\quad j=0,\\dots,n-1\\,, \\tag{6a}\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial J_\\lambda(\\mathbf{w},b)}{\\partial b} = \\frac{1}{m}\\sum_{i=0}^{m-1} \\Big(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}\\Big)\\,. \\tag{6b}\n",
        "$$\n",
        "Para **L1**, usa-se **subgradiente** nos pesos:\n",
        "$$\n",
        "\\partial_{w_j} J_\\lambda^{(L1)}(\\mathbf{w},b) \\;\\ni\\; \\frac{1}{m}\\sum_{i=0}^{m-1} \\Big(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}\\Big)\\,x_{j}^{(i)} \\;+\\; \\frac{\\lambda}{m}\\,\\operatorname{sgn}(w_j)\\,, \\tag{7}\n",
        "$$\n",
        "onde $\\operatorname{sgn}(0)\\in[-1,1]$. Em ambos os casos, o viés $b$ não é penalizado e mantém o gradiente da regressão logística sem regularização.\n",
        "\n",
        "### Ajuste de Parâmetros por Gradiente\n",
        "O esquema de atualização preserva a forma, acrescentando apenas o termo de penalização no passo de $\\mathbf{w}$ (L2) ou usando subgradientes (L1):\n",
        "$$\n",
        "\\begin{align*}\n",
        "& \\text{Repeat } \\{ \\\\\n",
        "& \\quad b \\;:=\\; b \\;-\\; \\alpha \\;\\frac{\\partial J_\\lambda(\\mathbf{w},b)}{\\partial b}, \\tag{8a} \\\\\n",
        "& \\quad w_j \\;:=\\; w_j \\;-\\; \\alpha \\;\\frac{\\partial J_\\lambda(\\mathbf{w},b)}{\\partial w_j}, \\quad j = 0,\\ldots,n-1. \\tag{8b}\\\\\n",
        "& \\}\n",
        "\\end{align*}\n",
        "$$\n",
        "A escolha de $\\lambda>0$ controla o compromisso viés–variância: maiores valores induzem pesos menores e fronteiras mais “suaves”.\n",
        "\n",
        "### Interpretação e Regra de Decisão\n",
        "A interpretação probabilística permanece: $f_{\\mathbf{w},b}(\\mathbf{x}) \\approx P(y=1\\mid \\mathbf{x})$. A regra de decisão é a mesma:\n",
        "- Se $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) \\geq 0.5$, então $\\hat{y}^{(i)}=1$; caso contrário, $\\hat{y}^{(i)}=0$.\n",
        "A regularização tende a reduzir a variância do classificador e pode melhorar desempenho fora da amostra, especialmente em cenários com muitas *features* ou colinearidade.\n",
        "\n",
        "### Resumo\n",
        "A regularização adiciona uma penalização aos pesos na função de custo: **L2** encolhe coeficientes de modo suave e estabiliza o problema; **L1** promove esparsidade, realizando seleção de variáveis. Em ambos os casos, o modelo logístico e sua interpretação probabilística permanecem, enquanto a complexidade é controlada via $\\lambda$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55442794-c671-4fa6-9cae-cde09fd93499",
      "metadata": {
        "id": "55442794-c671-4fa6-9cae-cde09fd93499",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Exercício 4: Preparação dos dados"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07ec864a-27f1-4b6c-ab56-5da68d4330a7",
      "metadata": {
        "id": "07ec864a-27f1-4b6c-ab56-5da68d4330a7"
      },
      "source": [
        "Para os próximos dois exercícios, vamos usar um conjunto de dados diferentes. Uma fábrica produz microchips e o objetivo do modelo de regressão logística é prever se um dado microchip passará pelos testes de qualidade (QA, de _quality assurance_)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "739a090e-895b-4737-83e7-911aa407d1ae",
      "metadata": {
        "id": "739a090e-895b-4737-83e7-911aa407d1ae"
      },
      "source": [
        "### Criando o conjunto de dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7ffc328-f0db-4012-bff7-87111e238ad0",
      "metadata": {
        "id": "c7ffc328-f0db-4012-bff7-87111e238ad0"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ./data\n",
        "!touch ./data/microchips.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a59577c-929d-47ff-990d-dd1fcb764eec",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "4a59577c-929d-47ff-990d-dd1fcb764eec"
      },
      "outputs": [],
      "source": [
        "microchips_data = \"\"\"0.051267,0.69956,1\n",
        "-0.092742,0.68494,1\n",
        "-0.21371,0.69225,1\n",
        "-0.375,0.50219,1\n",
        "-0.51325,0.46564,1\n",
        "-0.52477,0.2098,1\n",
        "-0.39804,0.034357,1\n",
        "-0.30588,-0.19225,1\n",
        "0.016705,-0.40424,1\n",
        "0.13191,-0.51389,1\n",
        "0.38537,-0.56506,1\n",
        "0.52938,-0.5212,1\n",
        "0.63882,-0.24342,1\n",
        "0.73675,-0.18494,1\n",
        "0.54666,0.48757,1\n",
        "0.322,0.5826,1\n",
        "0.16647,0.53874,1\n",
        "-0.046659,0.81652,1\n",
        "-0.17339,0.69956,1\n",
        "-0.47869,0.63377,1\n",
        "-0.60541,0.59722,1\n",
        "-0.62846,0.33406,1\n",
        "-0.59389,0.005117,1\n",
        "-0.42108,-0.27266,1\n",
        "-0.11578,-0.39693,1\n",
        "0.20104,-0.60161,1\n",
        "0.46601,-0.53582,1\n",
        "0.67339,-0.53582,1\n",
        "-0.13882,0.54605,1\n",
        "-0.29435,0.77997,1\n",
        "-0.26555,0.96272,1\n",
        "-0.16187,0.8019,1\n",
        "-0.17339,0.64839,1\n",
        "-0.28283,0.47295,1\n",
        "-0.36348,0.31213,1\n",
        "-0.30012,0.027047,1\n",
        "-0.23675,-0.21418,1\n",
        "-0.06394,-0.18494,1\n",
        "0.062788,-0.16301,1\n",
        "0.22984,-0.41155,1\n",
        "0.2932,-0.2288,1\n",
        "0.48329,-0.18494,1\n",
        "0.64459,-0.14108,1\n",
        "0.46025,0.012427,1\n",
        "0.6273,0.15863,1\n",
        "0.57546,0.26827,1\n",
        "0.72523,0.44371,1\n",
        "0.22408,0.52412,1\n",
        "0.44297,0.67032,1\n",
        "0.322,0.69225,1\n",
        "0.13767,0.57529,1\n",
        "-0.0063364,0.39985,1\n",
        "-0.092742,0.55336,1\n",
        "-0.20795,0.35599,1\n",
        "-0.20795,0.17325,1\n",
        "-0.43836,0.21711,1\n",
        "-0.21947,-0.016813,1\n",
        "-0.13882,-0.27266,1\n",
        "0.18376,0.93348,0\n",
        "0.22408,0.77997,0\n",
        "0.29896,0.61915,0\n",
        "0.50634,0.75804,0\n",
        "0.61578,0.7288,0\n",
        "0.60426,0.59722,0\n",
        "0.76555,0.50219,0\n",
        "0.92684,0.3633,0\n",
        "0.82316,0.27558,0\n",
        "0.96141,0.085526,0\n",
        "0.93836,0.012427,0\n",
        "0.86348,-0.082602,0\n",
        "0.89804,-0.20687,0\n",
        "0.85196,-0.36769,0\n",
        "0.82892,-0.5212,0\n",
        "0.79435,-0.55775,0\n",
        "0.59274,-0.7405,0\n",
        "0.51786,-0.5943,0\n",
        "0.46601,-0.41886,0\n",
        "0.35081,-0.57968,0\n",
        "0.28744,-0.76974,0\n",
        "0.085829,-0.75512,0\n",
        "0.14919,-0.57968,0\n",
        "-0.13306,-0.4481,0\n",
        "-0.40956,-0.41155,0\n",
        "-0.39228,-0.25804,0\n",
        "-0.74366,-0.25804,0\n",
        "-0.69758,0.041667,0\n",
        "-0.75518,0.2902,0\n",
        "-0.69758,0.68494,0\n",
        "-0.4038,0.70687,0\n",
        "-0.38076,0.91886,0\n",
        "-0.50749,0.90424,0\n",
        "-0.54781,0.70687,0\n",
        "0.10311,0.77997,0\n",
        "0.057028,0.91886,0\n",
        "-0.10426,0.99196,0\n",
        "-0.081221,1.1089,0\n",
        "0.28744,1.087,0\n",
        "0.39689,0.82383,0\n",
        "0.63882,0.88962,0\n",
        "0.82316,0.66301,0\n",
        "0.67339,0.64108,0\n",
        "1.0709,0.10015,0\n",
        "-0.046659,-0.57968,0\n",
        "-0.23675,-0.63816,0\n",
        "-0.15035,-0.36769,0\n",
        "-0.49021,-0.3019,0\n",
        "-0.46717,-0.13377,0\n",
        "-0.28859,-0.060673,0\n",
        "-0.61118,-0.067982,0\n",
        "-0.66302,-0.21418,0\n",
        "-0.59965,-0.41886,0\n",
        "-0.72638,-0.082602,0\n",
        "-0.83007,0.31213,0\n",
        "-0.72062,0.53874,0\n",
        "-0.59389,0.49488,0\n",
        "-0.48445,0.99927,0\n",
        "-0.0063364,0.99927,0\n",
        "0.63265,-0.030612,0\"\"\"\n",
        "\n",
        "with open('./data/microchips.csv', 'w') as f:\n",
        "    f.write(microchips_data)\n",
        "\n",
        "# Importando dados\n",
        "X_features = ['Microchip Test 1', 'Microchip Test 2']\n",
        "y_name = ['Pass']\n",
        "data = pd.read_csv(\"./data/microchips.csv\", header=None, names=X_features + y_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bca491b3-5755-4fdd-898d-d8ad049f3bfc",
      "metadata": {
        "id": "bca491b3-5755-4fdd-898d-d8ad049f3bfc"
      },
      "source": [
        "### 4.a) Crie a matriz de treino $X_{\\rm train}$, o vetor de variáveis alvo $y_{\\rm train}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6ac0a1b-31eb-428e-86ef-b245fcb8bf5f",
      "metadata": {
        "id": "a6ac0a1b-31eb-428e-86ef-b245fcb8bf5f"
      },
      "outputs": [],
      "source": [
        "# ---- INSIRA SEU CÓDIGO AQUI\n",
        "# ...\n",
        "# ----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfe31470-9f57-443c-a9a1-63b1522d36ad",
      "metadata": {
        "id": "bfe31470-9f57-443c-a9a1-63b1522d36ad"
      },
      "source": [
        "### 4b) Explore (rapidamente) os dados"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73260d18-4644-4363-af85-370a33201fd5",
      "metadata": {
        "id": "73260d18-4644-4363-af85-370a33201fd5"
      },
      "source": [
        "**Inspecione a tabela com funcionalidade da biblioteca pandas e faça um scatterplot dos dados, usando a coluna \"Admitted\" como estilo do marcador (cor, forma ou ambos).**\n",
        "\n",
        "_Obs: não gaste muito tempo neste exercício. Por outro lado, é sempre importante inspecionar rapidamente os dados._\n",
        "\n",
        "\n",
        "> Besides `display()`, you can use several other methods to inspect a pandas DataFrame:\n",
        ">\n",
        "> - `.head()`: Shows the first 5 rows (or a specified number) of the DataFrame. Useful for a quick look at the data structure.\n",
        "> - `.info()`: Prints a concise summary of the DataFrame, including the index dtype > and column dtypes, non-null values and memory usage.\n",
        "> - `.describe()`: Generates descriptive statistics of the DataFrame's numerical > columns, such as count, mean, standard deviation, minimum, and maximum.\n",
        "> - `.shape`: Returns a tuple representing the dimensionality of the DataFrame (rows, > columns)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7607146f-524e-4182-888b-bc4cdf9d8afb",
      "metadata": {
        "id": "7607146f-524e-4182-888b-bc4cdf9d8afb"
      },
      "outputs": [],
      "source": [
        "# ---- INSIRA SEU CÓDIGO AQUI\n",
        "# ...\n",
        "# ----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74377f5c-8b99-40e2-b8f9-2c2a70e2089a",
      "metadata": {
        "id": "74377f5c-8b99-40e2-b8f9-2c2a70e2089a"
      },
      "source": [
        "### 4.c) Crie um conjunto de features polinomiais até ordem 6, contendo todos os dados cruzados"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93870b71-af8e-4595-8d3b-62d126312e67",
      "metadata": {
        "id": "93870b71-af8e-4595-8d3b-62d126312e67"
      },
      "source": [
        "_Obs: Use o método `PolynomialFeatures` da biblioteca Scikit-learn. Use a opção `include_bias=False` para não introduzir uma feature constante._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "638cd7ef-8060-42e1-b14f-1c539e778228",
      "metadata": {
        "id": "638cd7ef-8060-42e1-b14f-1c539e778228"
      },
      "outputs": [],
      "source": [
        "# ---- INSIRA SEU CÓDIGO AQUI\n",
        "# ...\n",
        "# X_poly_train = ...\n",
        "# ----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc7f9fa6-3a23-476a-917d-ecdf095dc5f6",
      "metadata": {
        "id": "cc7f9fa6-3a23-476a-917d-ecdf095dc5f6",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Exercício 5: Regressão logística com regularização - Método do gradiente"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c8dd2bf-c5e8-4506-a3ed-c4c2beb8dfd0",
      "metadata": {
        "id": "8c8dd2bf-c5e8-4506-a3ed-c4c2beb8dfd0"
      },
      "source": [
        "A nova matriz de dados `X_poly_train` possui 27 features por linha, estando bem suscetível ao problema de _overfitting_. Neste exercício, vamos adaptar e rodar o método do gradiente para o caso com regularização. Ao longo do exercício, será possível reutilizar as funções definidas no exercício 2 e incluir a regularização adicionalmente."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea00d35a-19ea-4d7f-b644-ebf5f7d9b56e",
      "metadata": {
        "id": "ea00d35a-19ea-4d7f-b644-ebf5f7d9b56e"
      },
      "source": [
        "### 5.a) Implementando as funções"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd7468b1-4844-4876-8822-7eca1d8e53a3",
      "metadata": {
        "id": "fd7468b1-4844-4876-8822-7eca1d8e53a3"
      },
      "source": [
        "Será necessário definir as seguintes funções:\n",
        "\n",
        "- `compute_cost_reg` implementa a equação (3) acima;\n",
        "- `compute_gradient_reg` implementa as equações (6a) e (6b) acima;\n",
        "- `gradient_descent` é idêntico ao método desenvolvido no Exercício 2, exceto pela modificação das chamadas às duas funções acima."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbd85a1c-473d-45e6-aaae-f81f5de163ce",
      "metadata": {
        "id": "bbd85a1c-473d-45e6-aaae-f81f5de163ce",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def compute_cost_reg(X, y, w, b, lambda_ = 1):\n",
        "    \"\"\"\n",
        "    Computes the cost over all examples.\n",
        "\n",
        "    Args:\n",
        "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
        "      y : (ndarray Shape (m,))  target value\n",
        "      w : (ndarray Shape (n,))  values of parameters of the model\n",
        "      b : (scalar)              value of bias parameter of the model\n",
        "      lambda_ : (scalar, float) Controls amount of regularization\n",
        "\n",
        "    Returns:\n",
        "      total_cost : (scalar)     cost\n",
        "    \"\"\"\n",
        "\n",
        "    # ---- INSIRA SEU CÓDIGO AQUI\n",
        "    # ...\n",
        "    # ----\n",
        "\n",
        "    return cost\n",
        "\n",
        "\n",
        "def compute_gradient_reg(X, y, w, b, lambda_ = 1):\n",
        "    \"\"\"\n",
        "    Computes the gradient for logistic regression with regularization\n",
        "\n",
        "    Args:\n",
        "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
        "      y : (ndarray Shape (m,))  target value\n",
        "      w : (ndarray Shape (n,))  values of parameters of the model\n",
        "      b : (scalar)              value of bias parameter of the model\n",
        "      lambda_ : (scalar,float)  regularization constant\n",
        "    Returns\n",
        "      dj_db : (scalar)             The gradient of the cost w.r.t. the parameter b.\n",
        "      dj_dw : (ndarray Shape (n,)) The gradient of the cost w.r.t. the parameters w.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # ---- INSIRA SEU CÓDIGO AQUI\n",
        "    # ...\n",
        "    # ----\n",
        "\n",
        "    return dj_dw, dj_db\n",
        "\n",
        "\n",
        "def gradient_descent(X, y, w_in, b_in, alpha, num_iters, lambda_):\n",
        "    \"\"\"\n",
        "    Performs batch gradient descent to learn w and b. Updates w and b by taking\n",
        "    num_iters gradient steps with learning rate alpha\n",
        "\n",
        "    Args:\n",
        "      X (ndarray (m, n)) : Data, m examples with n features each\n",
        "      y (ndarray (m,))   : target values\n",
        "      w_in (ndarray (n,)): initial model weight parameters\n",
        "      b_in (scalar)      : initial model bias parameter\n",
        "      alpha (float)      : Learning rate\n",
        "      num_iters (int)    : number of iterations to run gradient descent\n",
        "\n",
        "    Returns:\n",
        "      w (ndarray (n,))                   : Updated values of parameters\n",
        "      b (scalar)                         : Updated value of parameter\n",
        "      J_history (ndarray (num_iters,))   : History of cost values\n",
        "      w_history (ndarray (num_iters, n)) : History of parameters w\n",
        "      b_history (ndarray (num_iters,))   : History of parameters b\n",
        "      \"\"\"\n",
        "\n",
        "    # ---- INSIRA SEU CÓDIGO AQUI\n",
        "    # ...\n",
        "    # ----\n",
        "\n",
        "    return w, b, J_history, w_history, b_history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a217ae98-9e46-4bc8-8d96-c81e78269480",
      "metadata": {
        "id": "a217ae98-9e46-4bc8-8d96-c81e78269480"
      },
      "source": [
        "### 5.b) Treine o modelo, faça previsões e inspecione graficamente os resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63465684-e1ab-4594-93f7-f1abbc36c8ee",
      "metadata": {
        "id": "63465684-e1ab-4594-93f7-f1abbc36c8ee"
      },
      "source": [
        "Faça a regressão logística nos dados e verifique graficamente que o comportamento da função de custo está como o esperado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cce44635-7ab3-4446-ba3e-c2b063c9ea96",
      "metadata": {
        "id": "cce44635-7ab3-4446-ba3e-c2b063c9ea96"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# ---- INSIRA SEU CÓDIGO AQUI\n",
        "# ...\n",
        "# ----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ffa69da-7ce1-4dd2-a658-2019605e272b",
      "metadata": {
        "id": "2ffa69da-7ce1-4dd2-a658-2019605e272b",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Exercício 6: Regressão logística com regularização - Scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00bf1da8-3bbf-49f1-91c5-eb15487902c1",
      "metadata": {
        "id": "00bf1da8-3bbf-49f1-91c5-eb15487902c1"
      },
      "source": [
        "O objetivo deste exercício é obter os mesmos resultados do exercício acima usando a funcionalidade já existente no scikit-learn. Para tanto, usaremos novamente a função `PolynomialFeatures` para processar os dados de entrada `X_train`, seguida da função `LogisticRegression` para realizar o treinamento.\n",
        "\n",
        "Para combinar as operações, usaremos a função `Pipeline` que, como indica o nome, cria uma sequência de operações (i.e. um _pipeline_) definidas pelo usuário.\n",
        "\n",
        "_PS. Como as features já estão escalonadas, não usaremos o `StandardScaler` para renormalizar os dados._\n",
        "\n",
        "_PPS. Como criaremos novamente as features polinomiais, use `X_train` e não `X_poly_train` quando realizar o treinamento no ex 6.b)_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01f4f5ee-431a-406f-ad50-982076285a85",
      "metadata": {
        "id": "01f4f5ee-431a-406f-ad50-982076285a85"
      },
      "source": [
        "### 6.a) Defina um pipeline em scikit-learn que crie features polinomiais e defina um modelo de regressão logística com regularização 'ridge'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4d5b821-35d5-4abe-a5ef-7bc312f5e0e6",
      "metadata": {
        "id": "a4d5b821-35d5-4abe-a5ef-7bc312f5e0e6",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# ---- INSIRA SEU CÓDIGO AQUI\n",
        "# ...\n",
        "# ----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0698c34-69f3-4c20-b545-60bd2b0b59f6",
      "metadata": {
        "id": "f0698c34-69f3-4c20-b545-60bd2b0b59f6"
      },
      "source": [
        "### 6.b) Treine o modelo, faça previsões e inspecione graficamente os resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "712a9383-2e20-4c3e-91eb-3011638611c9",
      "metadata": {
        "id": "712a9383-2e20-4c3e-91eb-3011638611c9"
      },
      "outputs": [],
      "source": [
        "# ---- INSIRA SEU CÓDIGO AQUI\n",
        "# ...\n",
        "# ----"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}